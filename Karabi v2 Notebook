{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project Karani v2: Fine-tuning Gemma for Kiswahili Conversational AI\n## Data Science Process Walkthrough\n\n**Objective:** To develop \"Karani v2,\" a Kiswahili conversational AI clerk, by fine-tuning a Google Gemma model. This notebook follows a structured data science approach.\n\n**Methodology Inspired by PACE:**\n1.  **Plan & Understand:** Define goals, select model, outline methodology.\n2.  **Assemble Data:** Set up environment, configure parameters, prepare dataset.\n3.  **Code & Execute (Modeling & Training):** Load model, apply PEFT (QLoRA), train.\n4.  **Execute (Evaluation & Testing):** Qualitative testing.\n5.  **Deployment Considerations:** Outline next steps.\n\n**Model:** `google/gemma-2b-it`\n**Technique:** Parameter-Efficient Fine-Tuning (PEFT) with QLoRA.\n**Platform:** Kaggle Notebooks with GPU.","metadata":{}},{"cell_type":"markdown","source":"## PHASE 1: PLANNING & UNDERSTANDING (Continued) / PHASE 2: ASSEMBLE DATA (Start)\n\n### 2.0 Environment Setup: Installing Libraries\n\nWe begin by installing the essential Python libraries. These libraries provide the tools for accessing models, performing fine-tuning, managing data, and accelerating computations.\n* `transformers`: For models and tokenizers from Hugging Face.\n* `peft`: For Parameter-Efficient Fine-Tuning (PEFT) techniques like LoRA.\n* `trl`: For the `SFTTrainer`, which simplifies supervised fine-tuning.\n* `bitsandbytes`: For 4-bit quantization (QLoRA).\n* `accelerate`: To optimize PyTorch training on various hardware.\n* `datasets`: For loading and manipulating datasets.\n* `sentencepiece`: A tokenizer library often used by models like Gemma.","metadata":{}},{"cell_type":"code","source":"print(\"PHASE 2.0: Installing necessary libraries...\")\n!pip install -q transformers peft trl bitsandbytes accelerate datasets sentencepiece\nprint(\"Libraries installed successfully.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-02T07:00:23.321271Z","iopub.execute_input":"2025-06-02T07:00:23.321744Z","iopub.status.idle":"2025-06-02T07:01:43.651714Z","shell.execute_reply.started":"2025-06-02T07:00:23.321722Z","shell.execute_reply":"2025-06-02T07:01:43.650822Z"}},"outputs":[{"name":"stdout","text":"PHASE 2.0: Installing necessary libraries...\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m366.3/366.3 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mLibraries installed successfully.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### 2.0 Environment Setup: Importing Libraries & Initial Configuration\n\nWith the libraries installed, we now import the necessary modules. We also define some initial configuration parameters, such as the model name we'll be using and directory paths for outputs.\n","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import Dataset, DatasetDict # For handling our dummy dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    # pipeline, # We might use this later for inference, or direct generation\n    logging as hf_logging\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom trl import SFTTrainer\n\nprint(\"PHASE 2.0: Essential libraries imported.\")\n\n# Optional: Set logging verbosity for transformers to control the amount of output during model loading etc.\n# hf_logging.set_verbosity_error() # Use this for less verbose output\nhf_logging.set_verbosity_info() \nprint(f\"Transformers logging verbosity set to: {hf_logging.get_verbosity()}\")\n\n# ---\n# ### 2.1 Configuration\n# ---\nprint(\"PHASE 2.1: Defining initial configurations...\")\n\n# Model selection based on our plan\nmodel_name = \"google/gemma-2b-it\"\nprint(f\"Selected model: {model_name}\")\n\n# Directory for saving fine-tuning outputs on Kaggle\noutput_dir = \"./karani_v2_gemma_finetuned_kaggle_cell_by_cell\" \nfinal_checkpoint_dir = os.path.join(output_dir, \"final_checkpoint\")\n# Create the output directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\nprint(f\"Output directory set to: {output_dir}\")\nprint(f\"Final checkpoint directory will be: {final_checkpoint_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T07:05:15.375220Z","iopub.execute_input":"2025-06-02T07:05:15.375751Z","iopub.status.idle":"2025-06-02T07:05:43.152087Z","shell.execute_reply.started":"2025-06-02T07:05:15.375718Z","shell.execute_reply":"2025-06-02T07:05:43.151435Z"}},"outputs":[{"name":"stderr","text":"2025-06-02 07:05:28.270136: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748847928.452919      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748847928.507465      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"PHASE 2.0: Essential libraries imported.\nTransformers logging verbosity set to: 20\nPHASE 2.1: Defining initial configurations...\nSelected model: google/gemma-2b-it\nOutput directory set to: ./karani_v2_gemma_finetuned_kaggle_cell_by_cell\nFinal checkpoint directory will be: ./karani_v2_gemma_finetuned_kaggle_cell_by_cell/final_checkpoint\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### 2.2 Dataset Strategy & Preparation\n\nThe dataset is crucial for shaping Karani v2's abilities. It should consist of Kiswahili conversational exchanges. For the `SFTTrainer`, data is typically a list of conversations, where each conversation is a list of message dictionaries (containing `role` and `content`).\n\n**Note:** The following is a DUMMY dataset. It's essential to replace this with your actual, high-quality Kiswahili conversational data for meaningful fine-tuning.\n\n#### 2.2.1 Dummy Dataset for Development\nThis small dataset enables us to proceed with setting up the model loading and training pipeline.","metadata":{}},{"cell_type":"code","source":"print(\"PHASE 2.2.1: Defining dummy conversational dataset for development...\")\ndummy_conversations = [\n    {\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Habari Karani.\"},\n            {\"role\": \"assistant\", \"content\": \"Nzuri sana! Habari yako? Nikusaidie nini leo?\"}\n        ]\n    },\n    {\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Nataka kujua kuhusu mchakato wa kupata kitambulisho cha taifa.\"},\n            {\"role\": \"assistant\", \"content\": \"Elewa. Ili kupata kitambulisho cha taifa, unahitaji kuwa na cheti cha kuzaliwa na barua kutoka kwa mwenyekiti wa serikali za mitaa. Una hivi vitu?\"}\n        ]\n    },\n    {\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Asante kwa taarifa.\"},\n            {\"role\": \"assistant\", \"content\": \"Karibu sana! Kuna lingine naweza kukusaidia nalo?\"}\n        ]\n    },\n    # Example demonstrating Karani asking a question\n    {\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Nahitaji msaada kuhusu leseni.\"},\n            {\"role\": \"assistant\", \"content\": \"Sawa. Ungependa kujua kuhusu aina gani ya leseni? Ya udereva, biashara, au nyingine?\"}\n        ]\n    }\n]\nprint(f\"Defined {len(dummy_conversations)} dummy conversations.\")\n\n# Convert the list of conversations to Hugging Face Dataset format\n# This makes it easy to use with the Hugging Face Trainer.\n# We'll create a tiny 'train' and 'eval' split from this dummy data.\ntry:\n    train_data = Dataset.from_list(dummy_conversations[:3]) # Use first 3 for training\n    eval_data = Dataset.from_list(dummy_conversations[3:])  # Use last 1 for evaluation\n\n    # Create a DatasetDict to hold both splits\n    raw_datasets = DatasetDict({\n        \"train\": train_data,\n        \"eval\": eval_data\n    })\n    print(\"Dummy data converted to Hugging Face Dataset and DatasetDict objects successfully.\")\n    print(f\"Raw dummy datasets: {raw_datasets}\")\n    print(f\"Example training instance from Dataset: {raw_datasets['train'][0]}\")\nexcept Exception as e:\n    print(f\"Error creating Dataset objects: {e}\")\n\n# The SFTTrainer (which we'll set up later) can often handle the tokenization and formatting\n# internally if the tokenizer has a 'chat_template' and the dataset is structured correctly\n# (as a list of message dicts, which we've done here).\n# If manual formatting or more complex tokenization were needed, it would typically be performed in this section.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T07:21:06.220502Z","iopub.execute_input":"2025-06-02T07:21:06.221364Z","iopub.status.idle":"2025-06-02T07:21:06.251845Z","shell.execute_reply.started":"2025-06-02T07:21:06.221336Z","shell.execute_reply":"2025-06-02T07:21:06.251313Z"}},"outputs":[{"name":"stdout","text":"PHASE 2.2.1: Defining dummy conversational dataset for development...\nDefined 4 dummy conversations.\nDummy data converted to Hugging Face Dataset and DatasetDict objects successfully.\nRaw dummy datasets: DatasetDict({\n    train: Dataset({\n        features: ['messages'],\n        num_rows: 3\n    })\n    eval: Dataset({\n        features: ['messages'],\n        num_rows: 1\n    })\n})\nExample training instance from Dataset: {'messages': [{'content': 'Habari Karani.', 'role': 'user'}, {'content': 'Nzuri sana! Habari yako? Nikusaidie nini leo?', 'role': 'assistant'}]}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## PHASE 3: CODE & EXECUTE (MODELING)\n\n### 3.0 Load Base Model & Tokenizer\n\nWe now load the pre-trained `google/gemma-2b-it` model and its tokenizer. The tokenizer converts text data into a format the model understands (tokens), and vice-versa.\n\n#### 3.0.1 Quantization Configuration (QLoRA)\nTo fine-tune efficiently on Kaggle's limited GPU resources, we'll use QLoRA. This technique quantizes the large pre-trained model to 4-bits, significantly reducing its memory footprint. We then train small adapter layers on top of this quantized model.","metadata":{}},{"cell_type":"markdown","source":"### 3.0a Authenticating with Hugging Face Hub\n\nThe selected Gemma model (`google/gemma-2b-it`) is a \"gated\" model on Hugging Face. This requires users to agree to its terms of use on the model's Hugging Face page and then authenticate their environment to download it.\n\nWe'll use `notebook_login()` from the `huggingface_hub` library to log in. You'll need a Hugging Face account and an access token with at least 'read' permissions. You can create an access token in your Hugging Face account settings (Settings -> Access Tokens -> New token).","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nprint(\"Attempting Hugging Face login...\")\nprint(\"Please follow the prompts to log in. You will need an access token from your Hugging Face account.\")\nnotebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T07:30:35.563264Z","iopub.execute_input":"2025-06-02T07:30:35.563537Z","iopub.status.idle":"2025-06-02T07:30:35.579209Z","shell.execute_reply.started":"2025-06-02T07:30:35.563519Z","shell.execute_reply":"2025-06-02T07:30:35.578399Z"}},"outputs":[{"name":"stdout","text":"Attempting Hugging Face login...\nPlease follow the prompts to log in. You will need an access token from your Hugging Face account.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff7ddeb0257a4d99ab15fbff1081a8c0"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"print(f\"PHASE 3.0: Loading tokenizer for '{model_name}'...\")\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    print(\"Tokenizer loaded successfully.\")\n\n    # Gemma models might not have a pad_token set by default.\n    # Using the eos_token as pad_token is a common practice for autoregressive models.\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n        print(f\"Set tokenizer.pad_token to tokenizer.eos_token ('{tokenizer.eos_token}')\")\n    else:\n        print(f\"tokenizer.pad_token is already set to '{tokenizer.pad_token}'\")\n\nexcept Exception as e:\n    print(f\"Error loading tokenizer: {e}\")\n\nprint(\"\\nPHASE 3.0.1: Configuring BitsAndBytesConfig for QLoRA...\")\n# QLoRA configuration\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,                # Enable 4-bit quantization\n    bnb_4bit_quant_type=\"nf4\",        # Quantization type (nf4 is often recommended)\n    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16, # Compute dtype for operations.\n                                      # torch.bfloat16 is preferred on Ampere GPUs (like T4 on Kaggle if supported).\n                                      # Otherwise, torch.float16 is used.\n    bnb_4bit_use_double_quant=False,  # Optional: can save a bit more memory\n)\nprint(\"BitsAndBytesConfig for QLoRA configured.\")\nprint(f\"Using compute dtype: {bnb_config.bnb_4bit_compute_dtype}\")\n\n\nprint(f\"\\nPHASE 3.0.1: Loading base model '{model_name}' with QLoRA configuration...\")\ntry:\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=bnb_config,    # Apply QLoRA config\n        device_map=\"auto\",                 # Automatically map model to available GPU(s)\n                                           # This is crucial for large models on single or multiple GPUs.\n        trust_remote_code=True\n    )\n    print(\"Base model loaded successfully with QLoRA.\")\n\n    # Prepare model for k-bit training (enables gradient checkpointing, etc.)\n    # This is an important step when using bitsandbytes with PEFT for training stability and efficiency.\n    model = prepare_model_for_kbit_training(model)\n    print(\"Model prepared for k-bit training (e.g., gradient checkpointing enabled).\")\n    print(f\"Model loaded on device: {model.device}\")\n\nexcept Exception as e:\n    print(f\"Error loading model: {e}\")\n    # Fallback or error handling might be needed here in a production script\n    model = None # Ensure model is None if loading fails","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T07:30:58.841436Z","iopub.execute_input":"2025-06-02T07:30:58.842125Z","iopub.status.idle":"2025-06-02T07:31:25.251043Z","shell.execute_reply.started":"2025-06-02T07:30:58.842099Z","shell.execute_reply":"2025-06-02T07:31:25.250463Z"}},"outputs":[{"name":"stdout","text":"PHASE 3.0: Loading tokenizer for 'google/gemma-2b-it'...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a20ebce97a5147fe8a8900e1c73fa6de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3374c47d6b624894969f0973aa76469d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03b1b3bc4ca2497ab4f75a0221b0c6d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"682aea7b92164d6f92a87603fc90f945"}},"metadata":{}},{"name":"stderr","text":"loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.model\nloading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.json\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/special_tokens_map.json\nloading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer_config.json\nloading file chat_template.jinja from cache at None\n","output_type":"stream"},{"name":"stdout","text":"Tokenizer loaded successfully.\ntokenizer.pad_token is already set to '<pad>'\n\nPHASE 3.0.1: Configuring BitsAndBytesConfig for QLoRA...\nBitsAndBytesConfig for QLoRA configured.\nUsing compute dtype: torch.bfloat16\n\nPHASE 3.0.1: Loading base model 'google/gemma-2b-it' with QLoRA configuration...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a207268d87a34fedb4b7757d37eac21c"}},"metadata":{}},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\nModel config GemmaConfig {\n  \"architectures\": [\n    \"GemmaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 2,\n  \"eos_token_id\": 1,\n  \"head_dim\": 256,\n  \"hidden_act\": \"gelu\",\n  \"hidden_activation\": null,\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 16384,\n  \"max_position_embeddings\": 8192,\n  \"model_type\": \"gemma\",\n  \"num_attention_heads\": 8,\n  \"num_hidden_layers\": 18,\n  \"num_key_value_heads\": 1,\n  \"pad_token_id\": 0,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.51.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 256000\n}\n\nOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"763123a7fefb45f6ae5e0b2d3b2ebdec"}},"metadata":{}},{"name":"stderr","text":"loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/model.safetensors.index.json\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c945dc0176b40a6b20c28eaf3013f8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4868dd1e50614049b20c0e86aafcf2f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"048980730b72442392ba68ab78943262"}},"metadata":{}},{"name":"stderr","text":"Instantiating GemmaForCausalLM model under default dtype torch.float16.\nGenerate config GenerationConfig {\n  \"bos_token_id\": 2,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\ntarget_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea85b94884724933b679fa0cfe7c0a88"}},"metadata":{}},{"name":"stderr","text":"All model checkpoint weights were used when initializing GemmaForCausalLM.\n\nAll the weights of GemmaForCausalLM were initialized from the model checkpoint at google/gemma-2b-it.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use GemmaForCausalLM for predictions without further training.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b3c381f280a4bb78e77d89485025286"}},"metadata":{}},{"name":"stderr","text":"loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 2,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\n","output_type":"stream"},{"name":"stdout","text":"Base model loaded successfully with QLoRA.\nModel prepared for k-bit training (e.g., gradient checkpointing enabled).\nModel loaded on device: cuda:0\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"### 3.1 PEFT Configuration (LoRA)\n\nNow that the base model is loaded and quantized, we configure Low-Rank Adaptation (LoRA). LoRA freezes the weights of the large pre-trained model and injects smaller, trainable \"adapter\" layers into specified modules (typically the attention layers). Only these adapter layers are updated during fine-tuning, drastically reducing the number of trainable parameters and computational cost.","metadata":{}},{"cell_type":"code","source":"print(\"PHASE 3.1: Configuring LoRA...\")\n# LoRA configuration\nlora_config = LoraConfig(\n    r=16,                             # Rank of the update matrices. A common value, higher means more parameters.\n    lora_alpha=32,                    # Alpha parameter for LoRA scaling (often 2*r). Controls the magnitude of the LoRA update.\n    target_modules=[                  # List of module names in the model to apply LoRA to.\n                                      # These names are specific to the model architecture (Gemma in this case).\n                                      # Common targets are projection layers in attention blocks.\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\", # For MLP blocks\n        \"up_proj\",   # For MLP blocks\n        \"down_proj\"  # For MLP blocks\n    ],\n    lora_dropout=0.05,                # Dropout probability for LoRA layers to prevent overfitting.\n    bias=\"none\",                      # Specifies which bias parameters to train (\"none\", \"all\", or \"lora_only\"). \"none\" is common for LoRA.\n    task_type=\"CAUSAL_LM\"             # Specifies the task type (Causal Language Modeling for Gemma).\n)\nprint(\"LoraConfig created successfully.\")\n\ntry:\n    # Apply LoRA to the model\n    # This wraps the existing model with PEFTModel, adding the LoRA adapters.\n    model = get_peft_model(model, lora_config)\n    print(\"LoRA applied to the model successfully.\")\n\n    # Print the number of trainable parameters\n    # This will show a significantly smaller number compared to the full model size,\n    # highlighting the efficiency of LoRA.\n    model.print_trainable_parameters()\nexcept Exception as e:\n    print(f\"Error applying LoRA to the model: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T07:34:02.523479Z","iopub.execute_input":"2025-06-02T07:34:02.524114Z","iopub.status.idle":"2025-06-02T07:34:02.890160Z","shell.execute_reply.started":"2025-06-02T07:34:02.524092Z","shell.execute_reply":"2025-06-02T07:34:02.889382Z"}},"outputs":[{"name":"stdout","text":"PHASE 3.1: Configuring LoRA...\nLoraConfig created successfully.\nLoRA applied to the model successfully.\ntrainable params: 19,611,648 || all params: 2,525,784,064 || trainable%: 0.7765\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"### 3.2 Training Setup\n\nNow we define the parameters that will govern the training process itself.\n\n#### 3.2.1 Training Arguments\nThe `TrainingArguments` class from Hugging Face `transformers` allows us to specify various aspects of the training run, such as learning rate, batch size, number of epochs, logging preferences, and save strategies.","metadata":{}},{"cell_type":"code","source":"print(\"PHASE 3.2.1: Configuring TrainingArguments...\")\n\n# Check if bfloat16 is supported, otherwise default to float16 for fp16 argument\n# This is important for bnb_4bit_compute_dtype as well.\n# We already set bnb_config.bnb_4bit_compute_dtype based on this logic.\n# Here we ensure TrainingArguments fp16/bf16 flags are consistent.\nuse_bf16 = torch.cuda.is_bf16_supported()\n\ntraining_args = TrainingArguments(\n    output_dir=output_dir,                     # Directory to save model checkpoints and logs.\n    per_device_train_batch_size=1,             # Batch size per GPU for training. Small for large models on limited VRAM.\n    gradient_accumulation_steps=4,             # Accumulate gradients over N steps for a larger effective batch size.\n                                               # Effective batch size = 1 * 4 = 4.\n    learning_rate=2e-4,                        # Initial learning rate for the AdamW optimizer.\n    num_train_epochs=1,                        # Number of training epochs. Start with 1 for a quick test.\n                                               # For the dummy dataset, 1 epoch is more than enough.\n                                               # For a real dataset, you might need 2-3 epochs.\n    # max_steps=10,                            # Uncomment to run for a fixed number of steps (for very quick debugging).\n    logging_steps=5,                           # Log training metrics (e.g., loss) every N steps.\n    save_steps=10,                             # Save a model checkpoint every N steps. Adjust if max_steps is low.\n                                               # Since our dummy dataset is tiny, 10 steps might mean saving very frequently or at the end.\n    fp16=not use_bf16,                         # Use fp16 mixed precision if bf16 is not available.\n    bf16=use_bf16,                             # Use bf16 mixed precision if available (recommended for Ampere+ GPUs).\n    max_grad_norm=0.3,                         # Gradient clipping norm to prevent exploding gradients.\n    warmup_ratio=0.03,                         # Proportion of total training steps for a linear learning rate warmup.\n    lr_scheduler_type=\"constant_with_warmup\",  # Learning rate scheduler type. \"constant_with_warmup\" warms up then keeps LR constant.\n                                               # Other options: \"linear\", \"cosine\".\n    report_to=\"tensorboard\",                   # Log metrics to TensorBoard. Logs will be in `output_dir/runs`.\n    push_to_hub=False,                         # Set to True to automatically push model to Hugging Face Hub (requires login).\n    # optim=\"paged_adamw_32bit\",               # Optional: Paged AdamW optimizer can save memory, useful for QLoRA.\n                                               # Default AdamW (used if `optim` is not specified) is generally fine.\n)\nprint(\"TrainingArguments configured successfully.\")\nprint(f\"Using bf16: {use_bf16}, Using fp16: {not use_bf16}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T07:35:34.689756Z","iopub.execute_input":"2025-06-02T07:35:34.690385Z","iopub.status.idle":"2025-06-02T07:35:34.727852Z","shell.execute_reply.started":"2025-06-02T07:35:34.690361Z","shell.execute_reply":"2025-06-02T07:35:34.727214Z"}},"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\n","output_type":"stream"},{"name":"stdout","text":"PHASE 3.2.1: Configuring TrainingArguments...\nTrainingArguments configured successfully.\nUsing bf16: True, Using fp16: False\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"#### 3.2.2 SFTTrainer Initialization\nThe `SFTTrainer` from TRL (Transformer Reinforcement Learning library) simplifies the supervised fine-tuning process. It handles the complexities of data formatting (using the tokenizer's chat template if provided with appropriately structured data), tokenization, and managing the training loop with PEFT models.","metadata":{}},{"cell_type":"code","source":"print(\"PHASE 3.2.2: Initializing SFTTrainer...\")\n\ntry:\n    trainer = SFTTrainer(\n        model=model,                            # The PEFT-modified model (with LoRA adapters).\n        args=training_args,                     # Previously defined training arguments.\n        train_dataset=raw_datasets[\"train\"],    # Training dataset (our dummy Kiswahili conversations).\n        eval_dataset=raw_datasets[\"eval\"],      # Evaluation dataset (optional, but good for monitoring).\n        tokenizer=tokenizer,                    # Tokenizer.\n        peft_config=lora_config,                # Pass the LoRA config. SFTTrainer will handle the PEFT model.\n        dataset_kwargs={\"add_special_tokens\": False}, # Special tokens are typically handled by the chat template\n                                                # automatically applied by SFTTrainer if data is in 'messages' format.\n        # SFTTrainer will use the tokenizer's chat_template to format the 'messages' field \n        # from our dataset (e.g., dummy_conversations). This is crucial for instruction-tuned\n        # models like Gemma to understand the conversational structure.\n        # If your dataset was, for example, a single column of pre-formatted text, you might use:\n        # dataset_text_field=\"text_column_name_in_your_dataset\"\n        \n        # max_seq_length=1024,                  # Optional: Maximum sequence length for tokenization. \n                                                # SFTTrainer often infers this or uses the model's default.\n                                                # If your conversations are very long, you might need to set this.\n                                                # For Gemma 2B, max sequence length is 8192, but for fine-tuning,\n                                                # shorter lengths (e.g., 512, 1024, 2048) are common to save memory,\n                                                # depending on the average length of your training examples.\n    )\n    print(\"SFTTrainer initialized successfully.\")\nexcept Exception as e:\n    print(f\"Error initializing SFTTrainer: {e}\")\n    trainer = None # Ensure trainer is None if initialization fails","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T07:36:13.086642Z","iopub.execute_input":"2025-06-02T07:36:13.087232Z","iopub.status.idle":"2025-06-02T07:36:13.092665Z","shell.execute_reply.started":"2025-06-02T07:36:13.087209Z","shell.execute_reply":"2025-06-02T07:36:13.091772Z"}},"outputs":[{"name":"stdout","text":"PHASE 3.2.2: Initializing SFTTrainer...\nError initializing SFTTrainer: SFTTrainer.__init__() got an unexpected keyword argument 'tokenizer'\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"#### 3.2.2 SFTTrainer Initialization\nThe `SFTTrainer` from TRL (Transformer Reinforcement Learning library) simplifies the supervised fine-tuning process. It handles the complexities of data formatting (using the tokenizer's chat template if provided with appropriately structured data), tokenization, and managing the training loop with PEFT models.","metadata":{}},{"cell_type":"code","source":"print(\"PHASE 3.2.2: Initializing SFTTrainer (Revised Attempt)...\")\n\ntry:\n    trainer = SFTTrainer(\n        model=model,                            # The PEFT-modified model (with LoRA adapters).\n        args=training_args,                     # Previously defined training arguments.\n        train_dataset=raw_datasets[\"train\"],    # Training dataset (our dummy Kiswahili conversations).\n        eval_dataset=raw_datasets[\"eval\"],      # Evaluation dataset (optional, but good for monitoring).\n        # tokenizer=tokenizer,                  # <-- REMOVED THIS LINE\n        peft_config=lora_config,                # Pass the LoRA config. SFTTrainer will handle the PEFT model.\n        dataset_kwargs={\"add_special_tokens\": False}, # Special tokens are typically handled by the chat template\n                                                # automatically applied by SFTTrainer if data is in 'messages' format.\n        # SFTTrainer should use the tokenizer associated with the 'model' object.\n        # Gemma's tokenizer, when loaded with AutoTokenizer and then associated with the model,\n        # should have its chat_template used for formatting the 'messages' field.\n        \n        # max_seq_length=1024,                  # Optional: Maximum sequence length for tokenization.\n    )\n    print(\"SFTTrainer initialized successfully.\")\nexcept Exception as e:\n    print(f\"Error initializing SFTTrainer: {e}\")\n    trainer = None # Ensure trainer is None if initialization fails","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T07:49:51.750604Z","iopub.execute_input":"2025-06-02T07:49:51.750916Z","iopub.status.idle":"2025-06-02T07:49:51.756814Z","shell.execute_reply.started":"2025-06-02T07:49:51.750889Z","shell.execute_reply":"2025-06-02T07:49:51.756132Z"}},"outputs":[{"name":"stdout","text":"PHASE 3.2.2: Initializing SFTTrainer (Revised Attempt)...\nError initializing SFTTrainer: SFTTrainer.__init__() got an unexpected keyword argument 'dataset_kwargs'\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"print(\"PHASE 3.2.2: Initializing SFTTrainer (Second Revised Attempt)...\")\n\ntry:\n    trainer = SFTTrainer(\n        model=model,                            # The PEFT-modified model (with LoRA adapters).\n        args=training_args,                     # Previously defined training arguments.\n        train_dataset=raw_datasets[\"train\"],    # Training dataset (our dummy Kiswahili conversations).\n        eval_dataset=raw_datasets[\"eval\"],      # Evaluation dataset (optional, but good for monitoring).\n        # tokenizer=tokenizer,                  # REMOVED\n        peft_config=lora_config,                # Pass the LoRA config. SFTTrainer will handle the PEFT model.\n        # dataset_kwargs={\"add_special_tokens\": False}, # <-- REMOVED THIS LINE\n\n        # SFTTrainer should use the tokenizer associated with the 'model' object\n        # and apply its chat_template by default when the dataset is a list of 'messages' dicts.\n        \n        # max_seq_length=1024,                  # Optional: Maximum sequence length for tokenization.\n    )\n    print(\"SFTTrainer initialized successfully.\")\nexcept Exception as e:\n    print(f\"Error initializing SFTTrainer: {e}\")\n    trainer = None # Ensure trainer is None if initialization fails","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T07:52:16.637827Z","iopub.execute_input":"2025-06-02T07:52:16.638410Z","iopub.status.idle":"2025-06-02T07:52:20.224491Z","shell.execute_reply.started":"2025-06-02T07:52:16.638388Z","shell.execute_reply":"2025-06-02T07:52:20.223809Z"}},"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\n","output_type":"stream"},{"name":"stdout","text":"PHASE 3.2.2: Initializing SFTTrainer (Second Revised Attempt)...\n","output_type":"stream"},{"name":"stderr","text":"loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.model\nloading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer.json\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/special_tokens_map.json\nloading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/tokenizer_config.json\nloading file chat_template.jinja from cache at None\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Converting train dataset to ChatML:   0%|          | 0/3 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e25fc68a0ec142b592ca9e5322cd5f35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to train dataset:   0%|          | 0/3 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb9b1a12f5ad40c5b638099c3dbb0bbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/3 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef8c8b7c9582467ba1749e6fb12fe9d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/3 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c125860cfdee414192b14ec3eb0a4983"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Converting eval dataset to ChatML:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"194ce09e8c1740a4bca6b4875fc6d114"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to eval dataset:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dcd0b4c2c0a438b81e5532c1f1a7515"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing eval dataset:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c78892c4308c46e485ca21f22c41fbee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating eval dataset:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f40a99e30cf541c0bbd15883f5a724db"}},"metadata":{}},{"name":"stderr","text":"Using auto half precision backend\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"SFTTrainer initialized successfully.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## PHASE 4: CODE & EXECUTE (TRAINING)\n\n### 4.0 Start Fine-tuning Process\n\nWith the `SFTTrainer` initialized, we can now begin the fine-tuning process. The `trainer.train()` command will execute the training loop based on our configurations (dataset, epochs, learning rate, etc.).\n\n**Note:** Since we are currently using a very small DUMMY dataset, this training will complete very quickly (likely in a few seconds) and will not result in a meaningfully fine-tuned model. Its purpose here is to verify that the training pipeline runs without errors. For actual fine-tuning of Karani v2, this step will require your comprehensive Kiswahili conversational dataset and will take a significant amount of time.","metadata":{}},{"cell_type":"code","source":"print(\"PHASE 4.0: Starting fine-tuning process (with dummy data)...\")\n\nif trainer is not None:\n    try:\n        # This will run very quickly due to the tiny dummy dataset (3 training examples, 1 epoch).\n        # In a real scenario, this is the main training step and takes time.\n        training_results = trainer.train()\n        \n        print(\"Fine-tuning completed (with dummy data).\")\n        print(\"Training results summary (metrics might be limited with dummy data):\")\n        if training_results:\n            print(f\"  TrainOutput: {training_results}\")\n            # You can access specific metrics like training loss if needed, e.g.,\n            # if training_results.training_loss:\n            #     print(f\"  Final Training Loss: {training_results.training_loss}\")\n    except Exception as e:\n        print(f\"Error during training: {e}\")\nelse:\n    print(\"Trainer was not initialized successfully. Skipping training.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T07:54:09.075947Z","iopub.execute_input":"2025-06-02T07:54:09.076251Z","iopub.status.idle":"2025-06-02T07:54:14.762641Z","shell.execute_reply.started":"2025-06-02T07:54:09.076232Z","shell.execute_reply":"2025-06-02T07:54:14.762035Z"}},"outputs":[{"name":"stderr","text":"The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text. If text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 3\n  Num Epochs = 1\n  Instantaneous batch size per device = 1\n  Total train batch size (w. parallel, distributed & accumulation) = 4\n  Gradient Accumulation steps = 4\n  Total optimization steps = 1\n  Number of trainable parameters = 19,611,648\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"name":"stdout","text":"PHASE 4.0: Starting fine-tuning process (with dummy data)...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 00:01, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./karani_v2_gemma_finetuned_kaggle_cell_by_cell/checkpoint-1\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\nModel config GemmaConfig {\n  \"architectures\": [\n    \"GemmaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 2,\n  \"eos_token_id\": 1,\n  \"head_dim\": 256,\n  \"hidden_act\": \"gelu\",\n  \"hidden_activation\": null,\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 16384,\n  \"max_position_embeddings\": 8192,\n  \"model_type\": \"gemma\",\n  \"num_attention_heads\": 8,\n  \"num_hidden_layers\": 18,\n  \"num_key_value_heads\": 1,\n  \"pad_token_id\": 0,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.51.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 256000\n}\n\ntokenizer config file saved in ./karani_v2_gemma_finetuned_kaggle_cell_by_cell/checkpoint-1/tokenizer_config.json\nSpecial tokens file saved in ./karani_v2_gemma_finetuned_kaggle_cell_by_cell/checkpoint-1/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning completed (with dummy data).\nTraining results summary (metrics might be limited with dummy data):\n  TrainOutput: TrainOutput(global_step=1, training_loss=9.074804306030273, metrics={'train_runtime': 5.1343, 'train_samples_per_second': 0.584, 'train_steps_per_second': 0.195, 'total_flos': 1717283622912.0, 'train_loss': 9.074804306030273})\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"### 4.1 Save Fine-tuned Model (Adapters)\n\nAfter the training process concludes, it's essential to save the results. With PEFT methods like LoRA, we don't save the entire model again. Instead, we save the trained LoRA \"adapter\" layers. These adapters are small and can be loaded on top of the original pre-trained base model for inference. We also save the tokenizer.\n\n**Note:** For this run with dummy data, the saved adapters won't represent significant new learning. This step primarily verifies the model saving mechanism.","metadata":{}},{"cell_type":"code","source":"print(\"PHASE 4.1: Saving fine-tuned model adapters and tokenizer...\")\n\nif trainer is not None:\n    try:\n        # Ensure the final checkpoint directory exists\n        os.makedirs(final_checkpoint_dir, exist_ok=True)\n        print(f\"Ensured final checkpoint directory exists: {final_checkpoint_dir}\")\n\n        # Save the LoRA adapters trained by the SFTTrainer.\n        # trainer.model refers to the PeftModel.\n        trainer.model.save_pretrained(final_checkpoint_dir)\n        print(f\"Model LoRA adapters saved to: {final_checkpoint_dir}\")\n\n        # Save the tokenizer. This is important to ensure that the same tokenization\n        # settings used during training are used during inference.\n        tokenizer.save_pretrained(final_checkpoint_dir)\n        print(f\"Tokenizer saved to: {final_checkpoint_dir}\")\n        \n        print(\"Model adapters and tokenizer saved successfully.\")\n        \n        # You can list the files in the directory to confirm\n        print(f\"\\nFiles in {final_checkpoint_dir}:\")\n        for dirname, _, filenames in os.walk(final_checkpoint_dir):\n            for filename in filenames:\n                print(os.path.join(dirname, filename))\n\n    except Exception as e:\n        print(f\"Error saving model or tokenizer: {e}\")\nelse:\n    print(\"Trainer was not initialized successfully. Skipping model saving.\")\n\n# Reminder on how to load this later for inference:\n#\n# from peft import PeftModel\n# from transformers import AutoModelForCausalLM, AutoTokenizer\n#\n# # 1. Load the base model with quantization (same as in Cell 5)\n# # model_name = \"google/gemma-2b-it\"\n# # bnb_config = BitsAndBytesConfig(...) # As defined in Cell 5\n# # base_model_for_inference = AutoModelForCausalLM.from_pretrained(\n# #     model_name,\n# #     quantization_config=bnb_config,\n# #     device_map=\"auto\",\n# #     trust_remote_code=True\n# # )\n#\n# # 2. Load the PEFT model by applying the saved adapters to the base model\n# # finetuned_model_with_adapters = PeftModel.from_pretrained(base_model_for_inference, final_checkpoint_dir)\n#\n# # 3. Load the tokenizer from the same directory\n# # tokenizer_for_inference = AutoTokenizer.from_pretrained(final_checkpoint_dir)\n#\n# # 'finetuned_model_with_adapters' and 'tokenizer_for_inference' would then be ready for use.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T07:55:27.627022Z","iopub.execute_input":"2025-06-02T07:55:27.627314Z","iopub.status.idle":"2025-06-02T07:55:28.757715Z","shell.execute_reply.started":"2025-06-02T07:55:27.627295Z","shell.execute_reply":"2025-06-02T07:55:28.757161Z"}},"outputs":[{"name":"stdout","text":"PHASE 4.1: Saving fine-tuned model adapters and tokenizer...\nEnsured final checkpoint directory exists: ./karani_v2_gemma_finetuned_kaggle_cell_by_cell/final_checkpoint\n","output_type":"stream"},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\nModel config GemmaConfig {\n  \"architectures\": [\n    \"GemmaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 2,\n  \"eos_token_id\": 1,\n  \"head_dim\": 256,\n  \"hidden_act\": \"gelu\",\n  \"hidden_activation\": null,\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 16384,\n  \"max_position_embeddings\": 8192,\n  \"model_type\": \"gemma\",\n  \"num_attention_heads\": 8,\n  \"num_hidden_layers\": 18,\n  \"num_key_value_heads\": 1,\n  \"pad_token_id\": 0,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.51.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 256000\n}\n\ntokenizer config file saved in ./karani_v2_gemma_finetuned_kaggle_cell_by_cell/final_checkpoint/tokenizer_config.json\nSpecial tokens file saved in ./karani_v2_gemma_finetuned_kaggle_cell_by_cell/final_checkpoint/special_tokens_map.json\n","output_type":"stream"},{"name":"stdout","text":"Model LoRA adapters saved to: ./karani_v2_gemma_finetuned_kaggle_cell_by_cell/final_checkpoint\nTokenizer saved to: ./karani_v2_gemma_finetuned_kaggle_cell_by_cell/final_checkpoint\nModel adapters and tokenizer saved successfully.\n\nFiles in ./karani_v2_gemma_finetuned_kaggle_cell_by_cell/final_checkpoint:\n./karani_v2_gemma_finetuned_kaggle_cell_by_cell/final_checkpoint/tokenizer_config.json\n./karani_v2_gemma_finetuned_kaggle_cell_by_cell/final_checkpoint/special_tokens_map.json\n./karani_v2_gemma_finetuned_kaggle_cell_by_cell/final_checkpoint/tokenizer.json\n./karani_v2_gemma_finetuned_kaggle_cell_by_cell/final_checkpoint/adapter_config.json\n./karani_v2_gemma_finetuned_kaggle_cell_by_cell/final_checkpoint/adapter_model.safetensors\n./karani_v2_gemma_finetuned_kaggle_cell_by_cell/final_checkpoint/README.md\n./karani_v2_gemma_finetuned_kaggle_cell_by_cell/final_checkpoint/tokenizer.model\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## PHASE 5: EXECUTE (EVALUATION & TESTING)\n\n### 5.0 Inference Setup\n\nTo evaluate Karani v2, we first need to set up the mechanism for inference. This involves:\n1.  Loading the fine-tuned model. Since we used PEFT (LoRA), this means loading the base Gemma model again (quantized) and then applying our saved LoRA adapters to it.\n2.  Loading the tokenizer that was saved alongside the adapters.\n3.  Defining a function that takes a user's Kiswahili prompt, prepares it using the model's chat template, feeds it to the model, and decodes the generated response.\n\n**Note:** The model we are loading here has only been \"fine-tuned\" on our tiny dummy dataset. Therefore, its responses will primarily reflect the capabilities of the base `gemma-2b-it` model, perhaps with very minor (and likely not meaningful) changes from the dummy data. This step is to test the end-to-end pipeline of loading and inferring from a PEFT-tuned model.","metadata":{}},{"cell_type":"code","source":"from peft import PeftModel\n# Transformers AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, etc.\n# were already imported in Cell 3. If this cell is run in a new session,\n# ensure those imports are executed first. We assume they are still in scope.\n\nprint(\"PHASE 5.0: Setting up for inference...\")\n\n# --- Configuration for loading ---\n# These should match the configurations used during model loading in Cell 5\n# model_name = \"google/gemma-2b-it\" # Already defined in Cell 3\n# final_checkpoint_dir # Already defined in Cell 3\n# bnb_config # Already defined in Cell 5\n\n# It's good practice to ensure variables from previous cells are accessible.\n# If you are running cells non-sequentially, you might need to re-declare them or re-run previous cells.\nprint(f\"Will load base model: {model_name}\")\nprint(f\"Will load adapters from: {final_checkpoint_dir}\")\n\n# --- Load the base model with quantization (same as in Cell 5) ---\nprint(f\"\\nLoading base model '{model_name}' for inference with QLoRA config...\")\ntry:\n    # Ensure bnb_config is available from Cell 5\n    if 'bnb_config' not in globals():\n        print(\"bnb_config not found, re-defining. Make sure Cell 5 was run.\")\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n            bnb_4bit_use_double_quant=False,\n        )\n\n    base_model_for_inference = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=bnb_config,\n        device_map=\"auto\", # Maps model to GPU\n        trust_remote_code=True\n    )\n    print(\"Base model for inference loaded successfully.\")\n\n    # --- Load the PEFT model by applying the saved LoRA adapters ---\n    print(f\"Loading LoRA adapters from {final_checkpoint_dir} onto the base model...\")\n    # Ensure PeftModel is imported: from peft import PeftModel\n    inference_model = PeftModel.from_pretrained(base_model_for_inference, final_checkpoint_dir)\n    # The model is now ready for inference and includes the LoRA modifications.\n    # It should be automatically moved to the same device as the base model.\n    print(\"LoRA adapters loaded successfully. PEFT model is ready for inference.\")\n    print(f\"Inference model device: {inference_model.device}\")\n\n    # --- Load the tokenizer from the same directory as the adapters ---\n    print(f\"Loading tokenizer from {final_checkpoint_dir}...\")\n    inference_tokenizer = AutoTokenizer.from_pretrained(final_checkpoint_dir)\n    print(\"Tokenizer for inference loaded successfully.\")\n\nexcept Exception as e:\n    print(f\"Error during model or tokenizer loading for inference: {e}\")\n    inference_model = None\n    inference_tokenizer = None\n\n# --- Define the generation function ---\ndef generate_response_for_karani(input_prompt_text, llm_model, llm_tokenizer, max_new_tokens=150):\n    \"\"\"\n    Generates a response from the provided LLM model given a Kiswahili prompt.\n    Uses the chat template defined in the tokenizer.\n    \"\"\"\n    if llm_model is None or llm_tokenizer is None:\n        return \"Error: Model or tokenizer not loaded for inference.\"\n\n    # Prepare the input for the model using its chat template.\n    chat_conversation_history = [\n        {\"role\": \"user\", \"content\": input_prompt_text}\n    ]\n    \n    formatted_llm_prompt = llm_tokenizer.apply_chat_template(\n        chat_conversation_history, \n        tokenize=False, \n        add_generation_prompt=True # Crucial for instruction/chat models\n    )\n    \n    # Tokenize the formatted prompt and move to the model's device\n    inputs = llm_tokenizer(formatted_llm_prompt, return_tensors=\"pt\", padding=True).to(llm_model.device)\n    \n    # Generate response\n    with torch.no_grad(): # Disable gradient calculations for inference\n        outputs = llm_model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            eos_token_id=llm_tokenizer.eos_token_id,\n            pad_token_id=llm_tokenizer.pad_token_id if llm_tokenizer.pad_token_id is not None else llm_tokenizer.eos_token_id,\n            do_sample=True,\n            temperature=0.7,\n            top_k=50,\n            top_p=0.95\n        )\n    \n    response_text = llm_tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n    return response_text.strip()\n\nif inference_model and inference_tokenizer:\n    print(\"\\nInference function `generate_response_for_karani` is ready with the loaded fine-tuned model.\")\nelse:\n    print(\"\\nInference function `generate_response_for_karani` defined, but model/tokenizer loading failed earlier.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T08:00:36.868541Z","iopub.execute_input":"2025-06-02T08:00:36.869273Z","iopub.status.idle":"2025-06-02T08:00:46.510178Z","shell.execute_reply.started":"2025-06-02T08:00:36.869251Z","shell.execute_reply":"2025-06-02T08:00:46.509493Z"}},"outputs":[{"name":"stdout","text":"PHASE 5.0: Setting up for inference...\nWill load base model: google/gemma-2b-it\nWill load adapters from: ./karani_v2_gemma_finetuned_kaggle_cell_by_cell/final_checkpoint\n\nLoading base model 'google/gemma-2b-it' for inference with QLoRA config...\n","output_type":"stream"},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/config.json\nModel config GemmaConfig {\n  \"architectures\": [\n    \"GemmaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 2,\n  \"eos_token_id\": 1,\n  \"head_dim\": 256,\n  \"hidden_act\": \"gelu\",\n  \"hidden_activation\": null,\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 16384,\n  \"max_position_embeddings\": 8192,\n  \"model_type\": \"gemma\",\n  \"num_attention_heads\": 8,\n  \"num_hidden_layers\": 18,\n  \"num_key_value_heads\": 1,\n  \"pad_token_id\": 0,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.51.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 256000\n}\n\nOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/model.safetensors.index.json\nInstantiating GemmaForCausalLM model under default dtype torch.float16.\nGenerate config GenerationConfig {\n  \"bos_token_id\": 2,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\ntarget_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99ddbe2f8c544dbc9c9751423c142391"}},"metadata":{}},{"name":"stderr","text":"All model checkpoint weights were used when initializing GemmaForCausalLM.\n\nAll the weights of GemmaForCausalLM were initialized from the model checkpoint at google/gemma-2b-it.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use GemmaForCausalLM for predictions without further training.\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/96988410cbdaeb8d5093d1ebdc5a8fb563e02bad/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 2,\n  \"eos_token_id\": 1,\n  \"pad_token_id\": 0\n}\n\n","output_type":"stream"},{"name":"stdout","text":"Base model for inference loaded successfully.\nLoading LoRA adapters from ./karani_v2_gemma_finetuned_kaggle_cell_by_cell/final_checkpoint onto the base model...\n","output_type":"stream"},{"name":"stderr","text":"loading file tokenizer.model\nloading file tokenizer.json\nloading file added_tokens.json\nloading file special_tokens_map.json\nloading file tokenizer_config.json\nloading file chat_template.jinja\n","output_type":"stream"},{"name":"stdout","text":"LoRA adapters loaded successfully. PEFT model is ready for inference.\nInference model device: cuda:0\nLoading tokenizer from ./karani_v2_gemma_finetuned_kaggle_cell_by_cell/final_checkpoint...\nTokenizer for inference loaded successfully.\n\nInference function `generate_response_for_karani` is ready with the loaded fine-tuned model.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"### 5.1 Qualitative Testing (Sample Prompts)\n\nNow we'll interact with our loaded \"fine-tuned\" Karani v2 model using a few sample Kiswahili prompts. This provides a basic qualitative assessment of its ability to generate relevant and coherent responses.\n\n**Important Reminder:** The model was \"fine-tuned\" on an extremely small dummy dataset. Therefore, its responses in this test will largely reflect the capabilities and knowledge of the base `google/gemma-2b-it` model. Any influence from our dummy data will be minimal and likely not noticeable in terms of improved Kiswahili nuance or specific \"clerk\" persona. The primary purpose here is to test the end-to-end inference pipeline with the loaded adapters.","metadata":{}},{"cell_type":"code","source":"print(\"PHASE 5.1: Starting Qualitative Testing...\")\n\nif 'inference_model' in globals() and inference_model is not None and \\\n   'inference_tokenizer' in globals() and inference_tokenizer is not None:\n\n    test_prompts_kiswahili = [\n        \"Habari! Unaweza kunisaidia?\",\n        \"Niambie kuhusu huduma za kibenki.\",\n        \"Nimepoteza kitambulisho changu, nifanye nini?\",\n        \"Mchakato wa kuanzisha biashara ndogo ni upi?\",\n        \"Ningependa kufahamu zaidi kuhusu Karani.\" # A prompt about itself\n    ]\n\n    print(\"\\n--- Testing Karani v2 (with LoRA adapters from dummy data training) ---\")\n    for prompt in test_prompts_kiswahili:\n        print(f\"\\nUser: {prompt}\")\n        \n        # For debugging or clarity, let's see the formatted prompt that goes into the model\n        # chat_for_template_display = [{\"role\": \"user\", \"content\": prompt}]\n        # templated_prompt_for_log_display = inference_tokenizer.apply_chat_template(\n        #     chat_for_template_display, \n        #     tokenize=False, \n        #     add_generation_prompt=True\n        # )\n        # print(f\"Formatted Model Input (approx):\\n{templated_prompt_for_log_display}\") # Optional: uncomment to see full model input\n        \n        karani_response = generate_response_for_karani(\n            prompt, \n            inference_model, \n            inference_tokenizer,\n            max_new_tokens=150 # You can adjust max_new_tokens if needed\n        )\n        print(f\"Karani v2: {karani_response}\")\n\n    print(\"\\n--- End of Qualitative Test ---\")\n    print(\"NOTE: Responses are primarily from the base Gemma model due to dummy data fine-tuning.\")\n    print(\"For Karani's specialized behavior, fine-tuning with a quality Kiswahili conversational dataset is essential.\")\n\nelse:\n    print(\"Inference model or tokenizer was not loaded successfully in the previous step. Skipping qualitative testing.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T08:02:18.092217Z","iopub.execute_input":"2025-06-02T08:02:18.092825Z","iopub.status.idle":"2025-06-02T08:02:46.027881Z","shell.execute_reply.started":"2025-06-02T08:02:18.092805Z","shell.execute_reply":"2025-06-02T08:02:46.027080Z"}},"outputs":[{"name":"stdout","text":"PHASE 5.1: Starting Qualitative Testing...\n\n--- Testing Karani v2 (with LoRA adapters from dummy data training) ---\n\nUser: Habari! Unaweza kunisaidia?\nKarani v2: Tafadhali! Niliona kujibu kuchagua. Nije naona kujibu kujibu.\n\nKasali naona kujibu nini?\n\nUser: Niambie kuhusu huduma za kibenki.\nKarani v2: **Huduma za Kibenki**\n\nHuduma za kibenki, pia chamawa matukio, ni kima ya matukio mawili. Ni kama matukio, huduma ya kibenki ni matukio ya kusikiti, kujibu, kujibu au kujibu. Huduma za kibenki hutumika na madaraka ya kiba, kama vile, ukusudi au ukweli.\n\n**Mareke za Huduma za Kibenki**\n\n* **Usimbo la Matukio:** Huduma ya kibenki ni matukio ya kusikiti, kujibu au kujibu.\n* **Imani Kufa kujibu:**\n\nUser: Nimepoteza kitambulisho changu, nifanye nini?\nKarani v2: Nimepoteza kitambulisho changu, ni futikeza asili. Ni muntuka kujiwe kwatira ya mtafsi wa upole.\n\nUser: Mchakato wa kuanzisha biashara ndogo ni upi?\nKarani v2: Sasa ni ngumu kusifuza ku kuji ku kusifuza. Ni niliona kuji ku kuji.\n\nUser: Ningependa kufahamu zaidi kuhusu Karani.\nKarani v2: **Karani** ni lugha ya KiKongo, nchini Tanzania.\n\n**Historia:**\n\n* Karani alenea na nchiridiaa ya 1969, wakati wa uraishi wa Tanzania.\n* Mwaka wa Karani, Jomo Kenyatta, alindika kujiisha riko ya uraishi wa nchiridiaa.\n* Karani ilindewa na maeneo ya Tanga, Kajiria na Makamburanga.\n\n**Usalama wa KiKongo:**\n\n* Karani ni mji wa KiKongo, ambapo ni jina ya lugha ya KiKongo.\n* Neno la utuliwa na lugha ya KiK\n\n--- End of Qualitative Test ---\nNOTE: Responses are primarily from the base Gemma model due to dummy data fine-tuning.\nFor Karani's specialized behavior, fine-tuning with a quality Kiswahili conversational dataset is essential.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}